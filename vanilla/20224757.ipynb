{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c9c388",
   "metadata": {
    "id": "WTUYbQLlYUQ-",
    "papermill": {
     "duration": 0.004561,
     "end_time": "2023-11-30T08:43:40.035954",
     "exception": false,
     "start_time": "2023-11-30T08:43:40.031393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EE837B Advances in Convolutional Neural Networks (Fall 2023)\n",
    "## Programming Assignment\n",
    "Department of Electrical Engineering, KAIST\n",
    "\n",
    "- Course Instructor : Prof. Junmo Kim\n",
    "\n",
    "- Primary TA : Hyounguk Shon\n",
    "\n",
    "- For questions regarding this assignment, use the course Q&A board on KLMS.\n",
    "\n",
    "---\n",
    "\n",
    "In this programming assignment, you are asked to reproduce the [Knowledge Distillation (KD) algorithm](https://arxiv.org/abs/1503.02531) in PyTorch. Your python code should distill a pre-trained teacher model into a smaller student model using the CIFAR-100 dataset. Additionally, we have an optional challenge for bonus credits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44cf8d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T08:43:40.045709Z",
     "iopub.status.busy": "2023-11-30T08:43:40.045294Z",
     "iopub.status.idle": "2023-11-30T08:43:52.741831Z",
     "shell.execute_reply": "2023-11-30T08:43:52.740638Z"
    },
    "papermill": {
     "duration": 12.703754,
     "end_time": "2023-11-30T08:43:52.743899",
     "exception": false,
     "start_time": "2023-11-30T08:43:40.040145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\r\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\r\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.7.22)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\r\n",
      "Installing collected packages: gdown\r\n",
      "Successfully installed gdown-4.7.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef306dc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T08:43:52.754387Z",
     "iopub.status.busy": "2023-11-30T08:43:52.754078Z",
     "iopub.status.idle": "2023-11-30T08:43:56.739099Z",
     "shell.execute_reply": "2023-11-30T08:43:56.738260Z"
    },
    "id": "RxMR39kBUTnQ",
    "papermill": {
     "duration": 3.9929,
     "end_time": "2023-11-30T08:43:56.741430",
     "exception": false,
     "start_time": "2023-11-30T08:43:52.748530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import gdown\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c9109",
   "metadata": {
    "id": "GYfj23oOmOr6",
    "papermill": {
     "duration": 0.004221,
     "end_time": "2023-11-30T08:43:56.750496",
     "exception": false,
     "start_time": "2023-11-30T08:43:56.746275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define ResNet architecture\n",
    "Do not change this code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3b1640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T08:43:56.760933Z",
     "iopub.status.busy": "2023-11-30T08:43:56.760231Z",
     "iopub.status.idle": "2023-11-30T08:43:56.787392Z",
     "shell.execute_reply": "2023-11-30T08:43:56.786531Z"
    },
    "id": "FQdqM8z8ZM6l",
    "papermill": {
     "duration": 0.034593,
     "end_time": "2023-11-30T08:43:56.789440",
     "exception": false,
     "start_time": "2023-11-30T08:43:56.754847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, is_last=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, depth, num_filters, block_name=\"BasicBlock\", num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        assert (\n",
    "            depth - 2\n",
    "        ) % 6 == 0, \"When use basicblock, depth should be 6n+2, e.g. 20, 32, 44, 56, 110, 1202\"\n",
    "        n = (depth - 2) // 6\n",
    "        block = BasicBlock\n",
    "\n",
    "        self.inplanes = num_filters[0]\n",
    "        self.conv1 = nn.Conv2d(3, num_filters[0], kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_filters[0])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, num_filters[1], n)\n",
    "        self.layer2 = self._make_layer(block, num_filters[2], n, stride=2)\n",
    "        self.layer3 = self._make_layer(block, num_filters[3], n, stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(num_filters[3] * block.expansion, num_classes)\n",
    "        self.stage_channels = num_filters\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.inplanes,\n",
    "                    planes * block.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = list([])\n",
    "        layers.append(\n",
    "            block(self.inplanes, planes, stride, downsample, is_last=(blocks == 1))\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, is_last=(i == blocks - 1)))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_feat_modules(self):\n",
    "        feat_m = nn.ModuleList([])\n",
    "        feat_m.append(self.conv1)\n",
    "        feat_m.append(self.bn1)\n",
    "        feat_m.append(self.relu)\n",
    "        feat_m.append(self.layer1)\n",
    "        feat_m.append(self.layer2)\n",
    "        feat_m.append(self.layer3)\n",
    "        return feat_m\n",
    "\n",
    "    def get_bn_before_relu(self):\n",
    "        bn1 = self.layer1[-1].bn2\n",
    "        bn2 = self.layer2[-1].bn2\n",
    "        bn3 = self.layer3[-1].bn2\n",
    "        return [bn1, bn2, bn3]\n",
    "\n",
    "    def get_stage_channels(self):\n",
    "        return self.stage_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)  # 32x32\n",
    "        f0 = x\n",
    "\n",
    "        x, f1_pre = self.layer1(x)  # 32x32\n",
    "        f1 = x\n",
    "        x, f2_pre = self.layer2(x)  # 16x16\n",
    "        f2 = x\n",
    "        x, f3_pre = self.layer3(x)  # 8x8\n",
    "        f3 = x\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        avg = x.reshape(x.size(0), -1)\n",
    "        out = self.fc(avg)\n",
    "\n",
    "        return out\n",
    "\n",
    "def resnet8x4(**kwargs):\n",
    "    return ResNet(8, [32, 64, 128, 256], \"basicblock\", **kwargs)\n",
    "\n",
    "def resnet32x4(**kwargs):\n",
    "    return ResNet(32, [32, 64, 128, 256], \"basicblock\", **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee2ed4",
   "metadata": {
    "papermill": {
     "duration": 0.004297,
     "end_time": "2023-11-30T08:43:56.798239",
     "exception": false,
     "start_time": "2023-11-30T08:43:56.793942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Base Trainer Class\n",
    "Do not change this code block.\n",
    "\n",
    "The base trainer will automatically download and load pre-trained teacher weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87707b97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T08:43:56.808158Z",
     "iopub.status.busy": "2023-11-30T08:43:56.807857Z",
     "iopub.status.idle": "2023-11-30T08:43:56.819628Z",
     "shell.execute_reply": "2023-11-30T08:43:56.818799Z"
    },
    "id": "h-ukFLINYURx",
    "papermill": {
     "duration": 0.018864,
     "end_time": "2023-11-30T08:43:56.821434",
     "exception": false,
     "start_time": "2023-11-30T08:43:56.802570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseTrainer:\n",
    "    #pretrained_teacher_link = 'https://drive.google.com/uc?id=1Gh3Z8BZ62PGD7PQiFiwmU9vMwMpF5F46'    \n",
    "    pretrained_teacher_link = 'https://drive.google.com/uc?id=1li1w_V3-NxPDUC9uofxijDnTABkp9rhD'\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.teacher = resnet32x4(num_classes=100)\n",
    "        self.student = resnet8x4(num_classes=100)\n",
    "        gdown.download(self.pretrained_teacher_link, './resnet_32x4.pth', resume=True)\n",
    "        self.teacher.load_state_dict(torch.load(\"./resnet_32x4.pth\", map_location=\"cpu\")[\"model\"])\n",
    "\n",
    "        self.train_transform = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "                ])\n",
    "        self.test_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "                ])\n",
    "\n",
    "        self.train_set = datasets.CIFAR100('./data/', download=True, train=True, transform=self.train_transform)\n",
    "        self.test_set = datasets.CIFAR100('./data/', download=False, train=False, transform=self.test_transform)\n",
    "        self.test_dataloader = DataLoader(self.test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    def save_student_checkpoint(self, ckpt_path):\n",
    "        state_dict = self.student.state_dict()\n",
    "        torch.save(state_dict, ckpt_path)\n",
    "\n",
    "    def load_student_checkpoint(self, ckpt_path):\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        self.student.load_state_dict(state_dict)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_student(self):\n",
    "        self.student.cuda().eval()\n",
    "        n = 0\n",
    "        correct = 0\n",
    "        for image, target in self.test_dataloader:\n",
    "            image = image.cuda()\n",
    "            target = target.cuda()\n",
    "            output = self.student(image)\n",
    "            n += image.size(0)\n",
    "            correct += output.max(-1).indices.eq(target).sum().item()\n",
    "        accuracy = 100 * correct / n\n",
    "        return accuracy\n",
    "\n",
    "    def train_student(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe902e65",
   "metadata": {
    "id": "6_kBr92qYURi",
    "papermill": {
     "duration": 0.004228,
     "end_time": "2023-11-30T08:43:56.830100",
     "exception": false,
     "start_time": "2023-11-30T08:43:56.825872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b185cca",
   "metadata": {
    "id": "Whrm65g0OQ0V",
    "papermill": {
     "duration": 0.004458,
     "end_time": "2023-11-30T08:43:56.839014",
     "exception": false,
     "start_time": "2023-11-30T08:43:56.834556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this section, you need to implement training of the student model.\n",
    "Specifically, you need to implement the followings:\n",
    "\n",
    "1. Knowledge Distillation algorithm\n",
    "2. Training pipeline\n",
    "\n",
    "You are free to edit below skeleton code. You are not allowed to edit the model architecture and data augmentation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7bade26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T08:43:56.849377Z",
     "iopub.status.busy": "2023-11-30T08:43:56.849095Z",
     "iopub.status.idle": "2023-11-30T08:43:56.865631Z",
     "shell.execute_reply": "2023-11-30T08:43:56.864956Z"
    },
    "id": "Z5CJM2fOOQ0W",
    "outputId": "59a4ef5b-e95c-4a8a-d606-496a066d1cea",
    "papermill": {
     "duration": 0.02385,
     "end_time": "2023-11-30T08:43:56.867396",
     "exception": false,
     "start_time": "2023-11-30T08:43:56.843546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class KDLoss(nn.Module):\n",
    "#     def __init__(self, use_kldiv=False, T=4):\n",
    "#         super(KDLoss, self).__init__()\n",
    "#         self.use_kldiv = use_kldiv\n",
    "#         self.temperature = T\n",
    "#     def forward(self, logits, targets):\n",
    "#         soft_logits = nn.Softmax(dim=1)(logits / self.temperature)\n",
    "#         soft_targets = nn.Softmax(dim=1)(targets / self.temperature)\n",
    "#         if self.use_kldiv:\n",
    "#             kd_loss = nn.KLDivLoss(reduction=\"batchmean\")(torch.log(soft_logits), soft_targets) # KL divergence instead of cross entropy\n",
    "#         else:\n",
    "#             kd_loss = -torch.mean(torch.sum(soft_targets * torch.log(soft_logits), dim=1)) # Cross entropy loss\n",
    "#         return kd_loss\n",
    "\n",
    "class KDLoss(nn.Module):\n",
    "    def __init__(self, T=4.0):\n",
    "        super(KDLoss, self).__init__()\n",
    "        self.temperature = T\n",
    "    def forward(self, student_logits, teacher_logits):\n",
    "        soft_student = nn.Softmax(dim=1)(student_logits / self.temperature)\n",
    "        soft_teacher = nn.Softmax(dim=1)(teacher_logits / self.temperature)\n",
    "        kd_loss = nn.KLDivLoss(reduction=\"batchmean\")(torch.log(soft_student), soft_teacher) * (self.temperature)**2\n",
    "        return kd_loss\n",
    "        \n",
    "class KDTrainer(BaseTrainer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ### YOU MAY EDIT BELOW ###\n",
    "        self.train_dataloader = DataLoader(self.train_set, batch_size=64, shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "        self.init_lr = 0.05\n",
    "        self.max_epoch = 240\n",
    "        self.optimizer = optim.SGD(self.student.parameters(), lr=self.init_lr, momentum=0.9, weight_decay=5e-4)\n",
    "        #...\n",
    "        self.temperature = 4.0  # in original paper, range from 1 - 20\n",
    "        self.alpha = 0.9\n",
    "        self.beta = 1.0\n",
    "        self.lr_scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[150, 180, 210], gamma=0.1)\n",
    "        self.cl_loss = nn.CrossEntropyLoss()\n",
    "        self.kd_loss = KDLoss(T=self.temperature)\n",
    "        \n",
    "    def train_student(self):\n",
    "        #### IMPLEMENT TRAINING HERE ####\n",
    "        self.teacher.cuda().eval()\n",
    "        self.student.cuda().train()\n",
    "        #print(self.student)\n",
    "        self.train_loss_arr = []\n",
    "        self.val_acc_arr = []\n",
    "        for epoch in range(self.max_epoch):\n",
    "            print(len(self.train_dataloader.dataset))\n",
    "            #...\n",
    "            running_loss = 0.0\n",
    "            for batch_idx, (data, hard_targets) in enumerate(self.train_dataloader):\n",
    "                data, hard_targets = data.cuda(), hard_targets.cuda()\n",
    "                with torch.no_grad():\n",
    "                  teacher_logits = self.teacher(data) \n",
    "                \n",
    "                student_logits = self.student(data)\n",
    "                loss = self.alpha * self.cl_loss(nn.Softmax(dim=1)(student_logits), hard_targets) + self.beta * self.kd_loss(student_logits, teacher_logits)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item() # mean batch loss (1) + mean batch loss (2) + ....\n",
    "                \n",
    "            train_loss = running_loss / len(self.train_dataloader) # divided by number of batch\n",
    "            val_acc = self.evaluate_student()\n",
    "            self.train_loss_arr.append(train_loss)\n",
    "            self.val_acc_arr.append(val_acc)\n",
    "            print(\"Epoch: %d, Loss: %f\" % (epoch+1, train_loss)) \n",
    "            print(\"val_acc: \", val_acc)\n",
    "            self.lr_scheduler.step()\n",
    "        \n",
    "        # Save training history\n",
    "        np.savetxt('train_loss_arr.csv', self.train_loss_arr, delimiter=',')\n",
    "        np.savetxt('val_acc_arr.csv', self.val_acc_arr, delimiter=',')\n",
    "        # Learning curve visualization\n",
    "        self.train_loss_arr = np.loadtxt('train_loss_arr.csv', delimiter=',')\n",
    "        epoch_arr = np.arange(1, self.max_epoch+1)\n",
    "        plt.plot(epoch_arr, self.train_loss_arr)\n",
    "#         plt.plot(epoch_arr, self.val_acc_arr)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Training loss\")\n",
    "        plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2db2b0a",
   "metadata": {
    "id": "fMKPvEfFOQ0X",
    "papermill": {
     "duration": 0.004239,
     "end_time": "2023-11-30T08:43:56.876019",
     "exception": false,
     "start_time": "2023-11-30T08:43:56.871780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "You do not need to modify the code in this section except the checkpoint path (**CKPT_PATH**).\n",
    "\n",
    "TAs will reproduce the results of your report with submitted checkpoint.\n",
    "\n",
    "**Before submission, make sure to check that the following code can print evaluation on your student model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e198db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T08:43:56.885784Z",
     "iopub.status.busy": "2023-11-30T08:43:56.885510Z",
     "iopub.status.idle": "2023-11-30T10:56:32.159248Z",
     "shell.execute_reply": "2023-11-30T10:56:32.158194Z"
    },
    "id": "c2ipeuStOQ0X",
    "outputId": "bbd36403-6a39-4668-a777-36594296bfb8",
    "papermill": {
     "duration": 7955.280965,
     "end_time": "2023-11-30T10:56:32.161345",
     "exception": false,
     "start_time": "2023-11-30T08:43:56.880380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1li1w_V3-NxPDUC9uofxijDnTABkp9rhD\n",
      "To: /kaggle/working/resnet_32x4.pth\n",
      "100%|██████████| 59.6M/59.6M [00:00<00:00, 173MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:04<00:00, 34984128.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data/\n",
      "50000\n",
      "Epoch: 1, Loss: 13.376295\n",
      "val_acc:  14.05\n",
      "50000\n",
      "Epoch: 2, Loss: 12.714995\n",
      "val_acc:  19.21\n",
      "50000\n",
      "Epoch: 3, Loss: 11.923720\n",
      "val_acc:  27.15\n",
      "50000\n",
      "Epoch: 4, Loss: 11.361276\n",
      "val_acc:  29.99\n",
      "50000\n",
      "Epoch: 5, Loss: 10.940132\n",
      "val_acc:  33.47\n",
      "50000\n",
      "Epoch: 6, Loss: 10.539122\n",
      "val_acc:  34.3\n",
      "50000\n",
      "Epoch: 7, Loss: 10.255471\n",
      "val_acc:  39.62\n",
      "50000\n",
      "Epoch: 8, Loss: 10.012815\n",
      "val_acc:  42.31\n",
      "50000\n",
      "Epoch: 9, Loss: 9.848786\n",
      "val_acc:  44.29\n",
      "50000\n",
      "Epoch: 10, Loss: 9.665761\n",
      "val_acc:  45.77\n",
      "50000\n",
      "Epoch: 11, Loss: 9.522132\n",
      "val_acc:  45.68\n",
      "50000\n",
      "Epoch: 12, Loss: 9.399784\n",
      "val_acc:  45.49\n",
      "50000\n",
      "Epoch: 13, Loss: 9.278556\n",
      "val_acc:  47.58\n",
      "50000\n",
      "Epoch: 14, Loss: 9.212190\n",
      "val_acc:  47.98\n",
      "50000\n",
      "Epoch: 15, Loss: 9.095820\n",
      "val_acc:  48.86\n",
      "50000\n",
      "Epoch: 16, Loss: 9.055551\n",
      "val_acc:  49.19\n",
      "50000\n",
      "Epoch: 17, Loss: 8.983056\n",
      "val_acc:  50.26\n",
      "50000\n",
      "Epoch: 18, Loss: 8.912612\n",
      "val_acc:  52.39\n",
      "50000\n",
      "Epoch: 19, Loss: 8.859704\n",
      "val_acc:  52.62\n",
      "50000\n",
      "Epoch: 20, Loss: 8.801247\n",
      "val_acc:  51.13\n",
      "50000\n",
      "Epoch: 21, Loss: 8.782536\n",
      "val_acc:  53.73\n",
      "50000\n",
      "Epoch: 22, Loss: 8.753345\n",
      "val_acc:  51.69\n",
      "50000\n",
      "Epoch: 23, Loss: 8.699394\n",
      "val_acc:  51.79\n",
      "50000\n",
      "Epoch: 24, Loss: 8.653778\n",
      "val_acc:  52.35\n",
      "50000\n",
      "Epoch: 25, Loss: 8.658646\n",
      "val_acc:  52.9\n",
      "50000\n",
      "Epoch: 26, Loss: 8.556173\n",
      "val_acc:  52.95\n",
      "50000\n",
      "Epoch: 27, Loss: 8.564229\n",
      "val_acc:  55.31\n",
      "50000\n",
      "Epoch: 28, Loss: 8.553177\n",
      "val_acc:  54.59\n",
      "50000\n",
      "Epoch: 29, Loss: 8.550659\n",
      "val_acc:  56.8\n",
      "50000\n",
      "Epoch: 30, Loss: 8.523731\n",
      "val_acc:  55.03\n",
      "50000\n",
      "Epoch: 31, Loss: 8.511944\n",
      "val_acc:  55.6\n",
      "50000\n",
      "Epoch: 32, Loss: 8.448965\n",
      "val_acc:  55.59\n",
      "50000\n",
      "Epoch: 33, Loss: 8.460399\n",
      "val_acc:  55.98\n",
      "50000\n",
      "Epoch: 34, Loss: 8.449890\n",
      "val_acc:  54.29\n",
      "50000\n",
      "Epoch: 35, Loss: 8.460219\n",
      "val_acc:  54.52\n",
      "50000\n",
      "Epoch: 36, Loss: 8.392588\n",
      "val_acc:  53.68\n",
      "50000\n",
      "Epoch: 37, Loss: 8.382865\n",
      "val_acc:  55.26\n",
      "50000\n",
      "Epoch: 38, Loss: 8.407715\n",
      "val_acc:  56.49\n",
      "50000\n",
      "Epoch: 39, Loss: 8.356892\n",
      "val_acc:  53.8\n",
      "50000\n",
      "Epoch: 40, Loss: 8.330449\n",
      "val_acc:  54.64\n",
      "50000\n",
      "Epoch: 41, Loss: 8.362894\n",
      "val_acc:  55.74\n",
      "50000\n",
      "Epoch: 42, Loss: 8.315529\n",
      "val_acc:  56.12\n",
      "50000\n",
      "Epoch: 43, Loss: 8.329370\n",
      "val_acc:  56.09\n",
      "50000\n",
      "Epoch: 44, Loss: 8.347830\n",
      "val_acc:  54.76\n",
      "50000\n",
      "Epoch: 45, Loss: 8.288575\n",
      "val_acc:  56.42\n",
      "50000\n",
      "Epoch: 46, Loss: 8.325399\n",
      "val_acc:  56.8\n",
      "50000\n",
      "Epoch: 47, Loss: 8.297388\n",
      "val_acc:  56.61\n",
      "50000\n",
      "Epoch: 48, Loss: 8.273423\n",
      "val_acc:  54.99\n",
      "50000\n",
      "Epoch: 49, Loss: 8.256699\n",
      "val_acc:  57.32\n",
      "50000\n",
      "Epoch: 50, Loss: 8.308215\n",
      "val_acc:  54.7\n",
      "50000\n",
      "Epoch: 51, Loss: 8.259941\n",
      "val_acc:  55.75\n",
      "50000\n",
      "Epoch: 52, Loss: 8.243087\n",
      "val_acc:  57.18\n",
      "50000\n",
      "Epoch: 53, Loss: 8.257720\n",
      "val_acc:  56.22\n",
      "50000\n",
      "Epoch: 54, Loss: 8.261526\n",
      "val_acc:  56.35\n",
      "50000\n",
      "Epoch: 55, Loss: 8.246651\n",
      "val_acc:  53.3\n",
      "50000\n",
      "Epoch: 56, Loss: 8.201277\n",
      "val_acc:  52.92\n",
      "50000\n",
      "Epoch: 57, Loss: 8.252434\n",
      "val_acc:  54.9\n",
      "50000\n",
      "Epoch: 58, Loss: 8.243971\n",
      "val_acc:  53.82\n",
      "50000\n",
      "Epoch: 59, Loss: 8.238914\n",
      "val_acc:  54.53\n",
      "50000\n",
      "Epoch: 60, Loss: 8.235549\n",
      "val_acc:  56.32\n",
      "50000\n",
      "Epoch: 61, Loss: 8.240373\n",
      "val_acc:  55.91\n",
      "50000\n",
      "Epoch: 62, Loss: 8.207656\n",
      "val_acc:  54.88\n",
      "50000\n",
      "Epoch: 63, Loss: 8.205099\n",
      "val_acc:  58.33\n",
      "50000\n",
      "Epoch: 64, Loss: 8.188252\n",
      "val_acc:  56.65\n",
      "50000\n",
      "Epoch: 65, Loss: 8.215652\n",
      "val_acc:  53.15\n",
      "50000\n",
      "Epoch: 66, Loss: 8.227400\n",
      "val_acc:  58.34\n",
      "50000\n",
      "Epoch: 67, Loss: 8.213992\n",
      "val_acc:  56.25\n",
      "50000\n",
      "Epoch: 68, Loss: 8.183453\n",
      "val_acc:  56.42\n",
      "50000\n",
      "Epoch: 69, Loss: 8.186391\n",
      "val_acc:  56.17\n",
      "50000\n",
      "Epoch: 70, Loss: 8.184827\n",
      "val_acc:  55.72\n",
      "50000\n",
      "Epoch: 71, Loss: 8.113230\n",
      "val_acc:  52.46\n",
      "50000\n",
      "Epoch: 72, Loss: 8.201518\n",
      "val_acc:  53.2\n",
      "50000\n",
      "Epoch: 73, Loss: 8.157349\n",
      "val_acc:  57.8\n",
      "50000\n",
      "Epoch: 74, Loss: 8.194767\n",
      "val_acc:  55.72\n",
      "50000\n",
      "Epoch: 75, Loss: 8.162706\n",
      "val_acc:  56.73\n",
      "50000\n",
      "Epoch: 76, Loss: 8.160723\n",
      "val_acc:  57.21\n",
      "50000\n",
      "Epoch: 77, Loss: 8.178369\n",
      "val_acc:  58.62\n",
      "50000\n",
      "Epoch: 78, Loss: 8.188216\n",
      "val_acc:  55.59\n",
      "50000\n",
      "Epoch: 79, Loss: 8.146312\n",
      "val_acc:  54.57\n",
      "50000\n",
      "Epoch: 80, Loss: 8.192363\n",
      "val_acc:  55.76\n",
      "50000\n",
      "Epoch: 81, Loss: 8.150961\n",
      "val_acc:  56.82\n",
      "50000\n",
      "Epoch: 82, Loss: 8.140618\n",
      "val_acc:  56.25\n",
      "50000\n",
      "Epoch: 83, Loss: 8.163101\n",
      "val_acc:  56.92\n",
      "50000\n",
      "Epoch: 84, Loss: 8.166477\n",
      "val_acc:  55.7\n",
      "50000\n",
      "Epoch: 85, Loss: 8.165026\n",
      "val_acc:  59.83\n",
      "50000\n",
      "Epoch: 86, Loss: 8.136478\n",
      "val_acc:  59.19\n",
      "50000\n",
      "Epoch: 87, Loss: 8.160972\n",
      "val_acc:  55.29\n",
      "50000\n",
      "Epoch: 88, Loss: 8.139309\n",
      "val_acc:  56.48\n",
      "50000\n",
      "Epoch: 89, Loss: 8.156269\n",
      "val_acc:  57.38\n",
      "50000\n",
      "Epoch: 90, Loss: 8.089113\n",
      "val_acc:  54.87\n",
      "50000\n",
      "Epoch: 91, Loss: 8.118971\n",
      "val_acc:  56.77\n",
      "50000\n",
      "Epoch: 92, Loss: 8.133374\n",
      "val_acc:  57.71\n",
      "50000\n",
      "Epoch: 93, Loss: 8.149871\n",
      "val_acc:  54.95\n",
      "50000\n",
      "Epoch: 94, Loss: 8.137920\n",
      "val_acc:  55.9\n",
      "50000\n",
      "Epoch: 95, Loss: 8.118227\n",
      "val_acc:  58.48\n",
      "50000\n",
      "Epoch: 96, Loss: 8.112707\n",
      "val_acc:  58.14\n",
      "50000\n",
      "Epoch: 97, Loss: 8.165936\n",
      "val_acc:  56.88\n",
      "50000\n",
      "Epoch: 98, Loss: 8.085903\n",
      "val_acc:  55.04\n",
      "50000\n",
      "Epoch: 99, Loss: 8.166145\n",
      "val_acc:  55.51\n",
      "50000\n",
      "Epoch: 100, Loss: 8.081712\n",
      "val_acc:  58.7\n",
      "50000\n",
      "Epoch: 101, Loss: 8.091253\n",
      "val_acc:  56.5\n",
      "50000\n",
      "Epoch: 102, Loss: 8.110916\n",
      "val_acc:  56.04\n",
      "50000\n",
      "Epoch: 103, Loss: 8.088198\n",
      "val_acc:  56.31\n",
      "50000\n",
      "Epoch: 104, Loss: 8.095832\n",
      "val_acc:  58.4\n",
      "50000\n",
      "Epoch: 105, Loss: 8.113021\n",
      "val_acc:  58.81\n",
      "50000\n",
      "Epoch: 106, Loss: 8.098324\n",
      "val_acc:  56.85\n",
      "50000\n",
      "Epoch: 107, Loss: 8.102354\n",
      "val_acc:  54.79\n",
      "50000\n",
      "Epoch: 108, Loss: 8.106549\n",
      "val_acc:  55.02\n",
      "50000\n",
      "Epoch: 109, Loss: 8.036323\n",
      "val_acc:  56.05\n",
      "50000\n",
      "Epoch: 110, Loss: 8.102088\n",
      "val_acc:  57.39\n",
      "50000\n",
      "Epoch: 111, Loss: 8.054031\n",
      "val_acc:  58.03\n",
      "50000\n",
      "Epoch: 112, Loss: 8.057747\n",
      "val_acc:  57.37\n",
      "50000\n",
      "Epoch: 113, Loss: 8.093504\n",
      "val_acc:  55.07\n",
      "50000\n",
      "Epoch: 114, Loss: 8.133180\n",
      "val_acc:  57.46\n",
      "50000\n",
      "Epoch: 115, Loss: 8.095458\n",
      "val_acc:  58.68\n",
      "50000\n",
      "Epoch: 116, Loss: 8.090854\n",
      "val_acc:  57.41\n",
      "50000\n",
      "Epoch: 117, Loss: 8.084817\n",
      "val_acc:  56.44\n",
      "50000\n",
      "Epoch: 118, Loss: 8.099965\n",
      "val_acc:  56.58\n",
      "50000\n",
      "Epoch: 119, Loss: 8.063389\n",
      "val_acc:  56.04\n",
      "50000\n",
      "Epoch: 120, Loss: 8.063940\n",
      "val_acc:  59.07\n",
      "50000\n",
      "Epoch: 121, Loss: 8.050167\n",
      "val_acc:  57.06\n",
      "50000\n",
      "Epoch: 122, Loss: 8.094898\n",
      "val_acc:  56.78\n",
      "50000\n",
      "Epoch: 123, Loss: 8.100898\n",
      "val_acc:  59.09\n",
      "50000\n",
      "Epoch: 124, Loss: 8.085818\n",
      "val_acc:  57.82\n",
      "50000\n",
      "Epoch: 125, Loss: 8.100185\n",
      "val_acc:  57.81\n",
      "50000\n",
      "Epoch: 126, Loss: 8.060944\n",
      "val_acc:  58.34\n",
      "50000\n",
      "Epoch: 127, Loss: 8.095877\n",
      "val_acc:  57.25\n",
      "50000\n",
      "Epoch: 128, Loss: 8.053038\n",
      "val_acc:  56.38\n",
      "50000\n",
      "Epoch: 129, Loss: 8.088925\n",
      "val_acc:  55.41\n",
      "50000\n",
      "Epoch: 130, Loss: 8.035365\n",
      "val_acc:  57.45\n",
      "50000\n",
      "Epoch: 131, Loss: 8.103021\n",
      "val_acc:  54.0\n",
      "50000\n",
      "Epoch: 132, Loss: 8.026491\n",
      "val_acc:  55.25\n",
      "50000\n",
      "Epoch: 133, Loss: 8.029297\n",
      "val_acc:  55.83\n",
      "50000\n",
      "Epoch: 134, Loss: 8.049426\n",
      "val_acc:  58.14\n",
      "50000\n",
      "Epoch: 135, Loss: 8.070570\n",
      "val_acc:  55.73\n",
      "50000\n",
      "Epoch: 136, Loss: 8.070164\n",
      "val_acc:  56.07\n",
      "50000\n",
      "Epoch: 137, Loss: 8.064528\n",
      "val_acc:  56.48\n",
      "50000\n",
      "Epoch: 138, Loss: 8.086932\n",
      "val_acc:  56.59\n",
      "50000\n",
      "Epoch: 139, Loss: 8.027516\n",
      "val_acc:  56.05\n",
      "50000\n",
      "Epoch: 140, Loss: 8.073219\n",
      "val_acc:  55.98\n",
      "50000\n",
      "Epoch: 141, Loss: 8.044946\n",
      "val_acc:  56.78\n",
      "50000\n",
      "Epoch: 142, Loss: 8.038435\n",
      "val_acc:  57.48\n",
      "50000\n",
      "Epoch: 143, Loss: 8.068533\n",
      "val_acc:  58.86\n",
      "50000\n",
      "Epoch: 144, Loss: 8.100648\n",
      "val_acc:  59.04\n",
      "50000\n",
      "Epoch: 145, Loss: 8.052568\n",
      "val_acc:  56.5\n",
      "50000\n",
      "Epoch: 146, Loss: 8.032274\n",
      "val_acc:  58.29\n",
      "50000\n",
      "Epoch: 147, Loss: 8.042648\n",
      "val_acc:  56.77\n",
      "50000\n",
      "Epoch: 148, Loss: 8.077307\n",
      "val_acc:  57.67\n",
      "50000\n",
      "Epoch: 149, Loss: 8.042656\n",
      "val_acc:  55.62\n",
      "50000\n",
      "Epoch: 150, Loss: 8.034737\n",
      "val_acc:  58.43\n",
      "50000\n",
      "Epoch: 151, Loss: 6.710396\n",
      "val_acc:  68.34\n",
      "50000\n",
      "Epoch: 152, Loss: 6.431561\n",
      "val_acc:  69.96\n",
      "50000\n",
      "Epoch: 153, Loss: 6.310861\n",
      "val_acc:  69.97\n",
      "50000\n",
      "Epoch: 154, Loss: 6.213596\n",
      "val_acc:  70.32\n",
      "50000\n",
      "Epoch: 155, Loss: 6.170725\n",
      "val_acc:  70.48\n",
      "50000\n",
      "Epoch: 156, Loss: 6.109848\n",
      "val_acc:  70.57\n",
      "50000\n",
      "Epoch: 157, Loss: 6.051783\n",
      "val_acc:  71.0\n",
      "50000\n",
      "Epoch: 158, Loss: 6.017861\n",
      "val_acc:  70.46\n",
      "50000\n",
      "Epoch: 159, Loss: 5.977765\n",
      "val_acc:  70.99\n",
      "50000\n",
      "Epoch: 160, Loss: 5.937821\n",
      "val_acc:  71.04\n",
      "50000\n",
      "Epoch: 161, Loss: 5.917557\n",
      "val_acc:  71.17\n",
      "50000\n",
      "Epoch: 162, Loss: 5.863837\n",
      "val_acc:  71.03\n",
      "50000\n",
      "Epoch: 163, Loss: 5.860705\n",
      "val_acc:  70.92\n",
      "50000\n",
      "Epoch: 164, Loss: 5.846505\n",
      "val_acc:  70.92\n",
      "50000\n",
      "Epoch: 165, Loss: 5.804827\n",
      "val_acc:  70.87\n",
      "50000\n",
      "Epoch: 166, Loss: 5.788267\n",
      "val_acc:  70.88\n",
      "50000\n",
      "Epoch: 167, Loss: 5.759532\n",
      "val_acc:  70.66\n",
      "50000\n",
      "Epoch: 168, Loss: 5.743710\n",
      "val_acc:  71.28\n",
      "50000\n",
      "Epoch: 169, Loss: 5.748001\n",
      "val_acc:  70.21\n",
      "50000\n",
      "Epoch: 170, Loss: 5.725275\n",
      "val_acc:  71.07\n",
      "50000\n",
      "Epoch: 171, Loss: 5.723496\n",
      "val_acc:  70.45\n",
      "50000\n",
      "Epoch: 172, Loss: 5.697318\n",
      "val_acc:  71.0\n",
      "50000\n",
      "Epoch: 173, Loss: 5.707595\n",
      "val_acc:  70.78\n",
      "50000\n",
      "Epoch: 174, Loss: 5.675088\n",
      "val_acc:  70.68\n",
      "50000\n",
      "Epoch: 175, Loss: 5.653151\n",
      "val_acc:  70.84\n",
      "50000\n",
      "Epoch: 176, Loss: 5.644416\n",
      "val_acc:  70.25\n",
      "50000\n",
      "Epoch: 177, Loss: 5.623771\n",
      "val_acc:  71.28\n",
      "50000\n",
      "Epoch: 178, Loss: 5.632199\n",
      "val_acc:  70.9\n",
      "50000\n",
      "Epoch: 179, Loss: 5.631174\n",
      "val_acc:  69.88\n",
      "50000\n",
      "Epoch: 180, Loss: 5.612796\n",
      "val_acc:  71.25\n",
      "50000\n",
      "Epoch: 181, Loss: 5.264226\n",
      "val_acc:  73.03\n",
      "50000\n",
      "Epoch: 182, Loss: 5.192759\n",
      "val_acc:  72.91\n",
      "50000\n",
      "Epoch: 183, Loss: 5.174589\n",
      "val_acc:  73.03\n",
      "50000\n",
      "Epoch: 184, Loss: 5.162226\n",
      "val_acc:  73.08\n",
      "50000\n",
      "Epoch: 185, Loss: 5.152616\n",
      "val_acc:  73.21\n",
      "50000\n",
      "Epoch: 186, Loss: 5.127768\n",
      "val_acc:  73.13\n",
      "50000\n",
      "Epoch: 187, Loss: 5.123093\n",
      "val_acc:  72.71\n",
      "50000\n",
      "Epoch: 188, Loss: 5.119127\n",
      "val_acc:  72.87\n",
      "50000\n",
      "Epoch: 189, Loss: 5.104691\n",
      "val_acc:  72.98\n",
      "50000\n",
      "Epoch: 190, Loss: 5.107925\n",
      "val_acc:  73.14\n",
      "50000\n",
      "Epoch: 191, Loss: 5.096952\n",
      "val_acc:  72.91\n",
      "50000\n",
      "Epoch: 192, Loss: 5.084453\n",
      "val_acc:  73.12\n",
      "50000\n",
      "Epoch: 193, Loss: 5.075585\n",
      "val_acc:  72.9\n",
      "50000\n",
      "Epoch: 194, Loss: 5.077412\n",
      "val_acc:  73.01\n",
      "50000\n",
      "Epoch: 195, Loss: 5.065562\n",
      "val_acc:  72.99\n",
      "50000\n",
      "Epoch: 196, Loss: 5.061840\n",
      "val_acc:  73.13\n",
      "50000\n",
      "Epoch: 197, Loss: 5.063634\n",
      "val_acc:  73.03\n",
      "50000\n",
      "Epoch: 198, Loss: 5.048225\n",
      "val_acc:  73.37\n",
      "50000\n",
      "Epoch: 199, Loss: 5.051877\n",
      "val_acc:  73.3\n",
      "50000\n",
      "Epoch: 200, Loss: 5.044281\n",
      "val_acc:  73.21\n",
      "50000\n",
      "Epoch: 201, Loss: 5.041452\n",
      "val_acc:  72.78\n",
      "50000\n",
      "Epoch: 202, Loss: 5.033644\n",
      "val_acc:  72.81\n",
      "50000\n",
      "Epoch: 203, Loss: 5.038701\n",
      "val_acc:  72.98\n",
      "50000\n",
      "Epoch: 204, Loss: 5.022284\n",
      "val_acc:  72.77\n",
      "50000\n",
      "Epoch: 205, Loss: 5.025641\n",
      "val_acc:  72.93\n",
      "50000\n",
      "Epoch: 206, Loss: 5.008382\n",
      "val_acc:  72.95\n",
      "50000\n",
      "Epoch: 207, Loss: 5.018306\n",
      "val_acc:  72.86\n",
      "50000\n",
      "Epoch: 208, Loss: 5.008667\n",
      "val_acc:  73.08\n",
      "50000\n",
      "Epoch: 209, Loss: 5.006622\n",
      "val_acc:  72.76\n",
      "50000\n",
      "Epoch: 210, Loss: 4.998333\n",
      "val_acc:  73.08\n",
      "50000\n",
      "Epoch: 211, Loss: 4.970077\n",
      "val_acc:  73.21\n",
      "50000\n",
      "Epoch: 212, Loss: 4.947143\n",
      "val_acc:  73.35\n",
      "50000\n",
      "Epoch: 213, Loss: 4.956922\n",
      "val_acc:  73.2\n",
      "50000\n",
      "Epoch: 214, Loss: 4.950773\n",
      "val_acc:  73.14\n",
      "50000\n",
      "Epoch: 215, Loss: 4.949449\n",
      "val_acc:  73.09\n",
      "50000\n",
      "Epoch: 216, Loss: 4.958959\n",
      "val_acc:  73.26\n",
      "50000\n",
      "Epoch: 217, Loss: 4.948690\n",
      "val_acc:  73.22\n",
      "50000\n",
      "Epoch: 218, Loss: 4.956444\n",
      "val_acc:  73.2\n",
      "50000\n",
      "Epoch: 219, Loss: 4.934979\n",
      "val_acc:  73.19\n",
      "50000\n",
      "Epoch: 220, Loss: 4.954681\n",
      "val_acc:  73.13\n",
      "50000\n",
      "Epoch: 221, Loss: 4.952253\n",
      "val_acc:  73.19\n",
      "50000\n",
      "Epoch: 222, Loss: 4.947387\n",
      "val_acc:  72.98\n",
      "50000\n",
      "Epoch: 223, Loss: 4.943775\n",
      "val_acc:  73.25\n",
      "50000\n",
      "Epoch: 224, Loss: 4.952061\n",
      "val_acc:  73.16\n",
      "50000\n",
      "Epoch: 225, Loss: 4.945441\n",
      "val_acc:  73.13\n",
      "50000\n",
      "Epoch: 226, Loss: 4.952447\n",
      "val_acc:  73.16\n",
      "50000\n",
      "Epoch: 227, Loss: 4.946496\n",
      "val_acc:  73.0\n",
      "50000\n",
      "Epoch: 228, Loss: 4.938510\n",
      "val_acc:  73.17\n",
      "50000\n",
      "Epoch: 229, Loss: 4.945342\n",
      "val_acc:  73.19\n",
      "50000\n",
      "Epoch: 230, Loss: 4.942226\n",
      "val_acc:  73.02\n",
      "50000\n",
      "Epoch: 231, Loss: 4.933585\n",
      "val_acc:  73.16\n",
      "50000\n",
      "Epoch: 232, Loss: 4.941448\n",
      "val_acc:  73.14\n",
      "50000\n",
      "Epoch: 233, Loss: 4.940072\n",
      "val_acc:  73.29\n",
      "50000\n",
      "Epoch: 234, Loss: 4.940659\n",
      "val_acc:  73.24\n",
      "50000\n",
      "Epoch: 235, Loss: 4.937035\n",
      "val_acc:  73.14\n",
      "50000\n",
      "Epoch: 236, Loss: 4.930727\n",
      "val_acc:  73.23\n",
      "50000\n",
      "Epoch: 237, Loss: 4.937660\n",
      "val_acc:  73.12\n",
      "50000\n",
      "Epoch: 238, Loss: 4.937250\n",
      "val_acc:  73.21\n",
      "50000\n",
      "Epoch: 239, Loss: 4.938403\n",
      "val_acc:  73.2\n",
      "50000\n",
      "Epoch: 240, Loss: 4.933741\n",
      "val_acc:  73.11\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CKPT_PATH = \"./student_checkpoint.pth\"\n",
    "\n",
    "trainer = KDTrainer()\n",
    "trainer.train_student()\n",
    "trainer.save_student_checkpoint(CKPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f06b9ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T10:56:32.220549Z",
     "iopub.status.busy": "2023-11-30T10:56:32.220166Z",
     "iopub.status.idle": "2023-11-30T10:56:35.376680Z",
     "shell.execute_reply": "2023-11-30T10:56:35.375525Z"
    },
    "papermill": {
     "duration": 3.189074,
     "end_time": "2023-11-30T10:56:35.379047",
     "exception": false,
     "start_time": "2023-11-30T10:56:32.189973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student model test accuracy: 73.110 %\n",
      "Is above threshold performance? True\n"
     ]
    }
   ],
   "source": [
    "trainer.load_student_checkpoint(CKPT_PATH)\n",
    "accuracy = trainer.evaluate_student()\n",
    "\n",
    "print(f\"Student model test accuracy: {accuracy:.3f} %\")\n",
    "print(f\"Is above threshold performance? {accuracy > 72.2}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7980.53461,
   "end_time": "2023-11-30T10:56:37.349515",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-30T08:43:36.814905",
   "version": "2.4.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
